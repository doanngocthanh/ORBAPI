import cv2
import numpy as np
import matplotlib.pyplot as plt

class ORBImageAligner:
    """
    Class ƒë·ªÉ th·ª±c hi·ªán alignment ·∫£nh s·ª≠ d·ª•ng ORB features v·ªõi size normalization
    X·ª≠ l√Ω ho√†n to√†n trong b·ªô nh·ªõ, kh√¥ng l∆∞u file
    """
    
    def __init__(self, target_dimension=800, orb_features=2000):
        """
        Kh·ªüi t·∫°o ORB Image Aligner
        
        Args:
            target_dimension (int): K√≠ch th∆∞·ªõc chu·∫©n ƒë·ªÉ normalize (default: 800)
            orb_features (int): S·ªë l∆∞·ª£ng ORB features t·ªëi ƒëa (default: 2000)
        """
        self.target_dimension = target_dimension
        self.orb_features = orb_features
        
        # Kh·ªüi t·∫°o ORB detector
        self.orb = cv2.ORB_create(
            nfeatures=orb_features,
            scaleFactor=1.2,
            nlevels=8,
            edgeThreshold=31,
            firstLevel=0,
            WTA_K=2,
            scoreType=cv2.ORB_HARRIS_SCORE,
            patchSize=31,
            fastThreshold=20
        )
        
        # RANSAC configurations for robustness
        self.ransac_configs = [
            {"threshold": 3.0, "maxIters": 3000, "confidence": 0.99},
            {"threshold": 5.0, "maxIters": 2000, "confidence": 0.995},
            {"threshold": 1.5, "maxIters": 5000, "confidence": 0.98},
        ]
        
    def normalize_size(self, base_img, target_img):
        """
        ƒê·ªìng nh·∫•t k√≠ch th∆∞·ªõc 2 ·∫£nh v·ªÅ c√πng scale
        
        Args:
            base_img: ·∫¢nh base
            target_img: ·∫¢nh target
            
        Returns:
            tuple: (base_normalized, target_normalized, base_scale, target_scale)
        """
        # T√≠nh scale cho base image
        base_h, base_w = base_img.shape[:2]
        base_max_dim = max(base_h, base_w)
        base_scale = self.target_dimension / base_max_dim
        
        # Resize base image
        base_new_w = int(base_w * base_scale)
        base_new_h = int(base_h * base_scale)
        base_normalized = cv2.resize(base_img, (base_new_w, base_new_h))
        
        # T√≠nh scale cho target image
        target_h, target_w = target_img.shape[:2]
        target_max_dim = max(target_h, target_w)
        target_scale = self.target_dimension / target_max_dim
        
        # Resize target image
        target_new_w = int(target_w * target_scale)
        target_new_h = int(target_h * target_scale)
        target_normalized = cv2.resize(target_img, (target_new_w, target_new_h))
        
        print(f"üîß Normalized sizes - Base: {base_normalized.shape}, Target: {target_normalized.shape}")
        
        return base_normalized, target_normalized, base_scale, target_scale
    
    def enhanced_preprocessing(self, img):
        """
        Preprocessing ƒë·ªÉ tƒÉng ch·∫•t l∆∞·ª£ng features
        
        Args:
            img: ·∫¢nh input
            
        Returns:
            numpy.ndarray: ·∫¢nh ƒë√£ ƒë∆∞·ª£c preprocessing
        """
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # CLAHE ƒë·ªÉ enhance contrast
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        enhanced = clahe.apply(gray)
        
        # Gaussian blur nh·∫π ƒë·ªÉ gi·∫£m noise
        blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)
        
        return blurred
    
    def detect_and_match_features(self, base_processed, target_processed):
        """
        Detect v√† match ORB features
        
        Args:
            base_processed: ·∫¢nh base ƒë√£ preprocessing
            target_processed: ·∫¢nh target ƒë√£ preprocessing
            
        Returns:
            tuple: (good_matches, keypoints1, keypoints2) ho·∫∑c None n·∫øu th·∫•t b·∫°i
        """
        # Detect features
        kp1, desc1 = self.orb.detectAndCompute(base_processed, None)
        kp2, desc2 = self.orb.detectAndCompute(target_processed, None)
        
        if desc1 is None or desc2 is None:
            return None
        
        print(f"‚ú® Features found - Base: {len(kp1)}, Target: {len(kp2)}")
        
        # Feature matching
        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
        matches = bf.knnMatch(desc1, desc2, k=2)
        
        # Lowe's ratio test
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < 0.75 * n.distance:
                    good_matches.append(m)
        
        print(f"üíé Good matches: {len(good_matches)}")
        
        if len(good_matches) < 10:
            return None
        
        return good_matches, kp1, kp2
    
    def find_robust_homography(self, good_matches, kp1, kp2):
        """
        T√¨m homography matrix robust v·ªõi multiple RANSAC attempts
        
        Args:
            good_matches: List c√°c good matches
            kp1: Keypoints c·ªßa ·∫£nh base
            kp2: Keypoints c·ªßa ·∫£nh target
            
        Returns:
            tuple: (best_matrix, best_inliers, best_mask) ho·∫∑c None n·∫øu th·∫•t b·∫°i
        """
        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        
        best_matrix = None
        best_inliers = 0
        best_mask = None
        
        for config in self.ransac_configs:
            matrix, mask = cv2.findHomography(
                dst_pts, src_pts, cv2.RANSAC,
                config["threshold"], maxIters=config["maxIters"], 
                confidence=config["confidence"]
            )
            
            if matrix is not None:
                inliers = np.sum(mask)
                print(f"  üìè Threshold {config['threshold']}: {inliers} inliers")
                
                if inliers > best_inliers:
                    best_matrix = matrix
                    best_inliers = inliers
                    best_mask = mask
        
        if best_matrix is None:
            return None
        
        print(f"‚úÖ Best result: {best_inliers} inliers")
        return best_matrix, best_inliers, best_mask
    
    def calculate_quality_score(self, base_img, aligned_img):
        """
        T√≠nh quality score cho aligned image
        
        Args:
            base_img: ·∫¢nh base
            aligned_img: ·∫¢nh ƒë√£ aligned
            
        Returns:
            float: Quality score (0-1)
        """
        try:
            # Convert to grayscale
            base_gray = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)
            aligned_gray = cv2.cvtColor(aligned_img, cv2.COLOR_BGR2GRAY)
            
            # Ensure same size
            h = min(base_gray.shape[0], aligned_gray.shape[0])
            w = min(base_gray.shape[1], aligned_gray.shape[1])
            base_gray = cv2.resize(base_gray, (w, h))
            aligned_gray = cv2.resize(aligned_gray, (w, h))
            
            # Template matching
            corr = cv2.matchTemplate(base_gray, aligned_gray, cv2.TM_CCORR_NORMED)[0][0]
            
            # Mean squared error
            mse = np.mean((base_gray.astype(np.float32) - aligned_gray.astype(np.float32))**2) / (255.0**2)
            
            # Combined score
            quality = corr * (1 - mse)
            
            return max(0, min(1, quality))
            
        except Exception:
            return 0.5
    
    def create_comparison_image(self, base_img, target_img, aligned_img):
        """
        T·∫°o ·∫£nh so s√°nh 3 ·∫£nh
        
        Args:
            base_img: ·∫¢nh base
            target_img: ·∫¢nh target
            aligned_img: ·∫¢nh aligned
            
        Returns:
            numpy.ndarray: ·∫¢nh comparison
        """
        height, width = 400, 300
        
        # Resize all to same size
        base_resized = cv2.resize(base_img, (width, height))
        target_resized = cv2.resize(target_img, (width, height))
        aligned_resized = cv2.resize(aligned_img, (width, height))
        
        # Add labels
        cv2.putText(base_resized, "BASE", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(target_resized, "TARGET", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        cv2.putText(aligned_resized, "ALIGNED", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
        
        # Combine horizontally
        comparison = np.hstack([base_resized, target_resized, aligned_resized])
        return comparison
    
    def create_visualization(self, base_norm, target_norm, kp1, kp2, good_matches, mask):
        """
        T·∫°o ·∫£nh visualization showing matches
        
        Args:
            base_norm: ·∫¢nh base normalized
            target_norm: ·∫¢nh target normalized  
            kp1: Keypoints base
            kp2: Keypoints target
            good_matches: Good matches
            mask: Inlier mask
            
        Returns:
            numpy.ndarray: Visualization image
        """
        # L·∫•y inlier matches
        inlier_matches = [good_matches[i] for i in range(len(good_matches)) if mask[i]]
        
        # Draw matches
        vis_image = cv2.drawMatches(
            base_norm, kp1,
            target_norm, kp2,
            inlier_matches[:30], None,
            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
        )
        
        return vis_image
    
    def align(self, base_img, target_img):
        """
        Th·ª±c hi·ªán alignment ch√≠nh - x·ª≠ l√Ω ho√†n to√†n trong b·ªô nh·ªõ
        
        Args:
            base_img: ·∫¢nh base (numpy array ho·∫∑c ƒë∆∞·ªùng d·∫´n file)
            target_img: ·∫¢nh target (numpy array ho·∫∑c ƒë∆∞·ªùng d·∫´n file)
            
        Returns:
            dict: K·∫øt qu·∫£ alignment v·ªõi c√°c ·∫£nh trong b·ªô nh·ªõ
        """
        try:
            # ƒê·ªçc ·∫£nh n·∫øu l√† ƒë∆∞·ªùng d·∫´n
            if isinstance(base_img, str):
                base_image_original = cv2.imread(base_img)
                if base_image_original is None:
                    return {"success": False, "error": "Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh base"}
            else:
                base_image_original = base_img.copy()
            
            if isinstance(target_img, str):
                target_image_original = cv2.imread(target_img)
                if target_image_original is None:
                    return {"success": False, "error": "Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh target"}
            else:
                target_image_original = target_img.copy()
            
            print(f"üìñ Original sizes - Base: {base_image_original.shape}, Target: {target_image_original.shape}")
            
            # Step 1: Size Normalization
            base_norm, target_norm, base_scale, target_scale = self.normalize_size(
                base_image_original, target_image_original
            )
            
            # Step 2: Enhanced preprocessing
            base_processed = self.enhanced_preprocessing(base_norm)
            target_processed = self.enhanced_preprocessing(target_norm)
            
            # Step 3: Feature detection and matching
            print("üîç ORB feature detection v·ªõi size ƒë√£ normalized...")
            match_result = self.detect_and_match_features(base_processed, target_processed)
            
            if match_result is None:
                return {"success": False, "error": "Kh√¥ng t√¨m th·∫•y ƒë·ªß features ho·∫∑c matches"}
            
            good_matches, kp1, kp2 = match_result
            
            # Step 4: Robust homography estimation  
            print("üéØ Robust homography estimation...")
            homography_result = self.find_robust_homography(good_matches, kp1, kp2)
            
            if homography_result is None:
                return {"success": False, "error": "Kh√¥ng th·ªÉ t√≠nh homography matrix"}
            
            best_matrix, best_inliers, best_mask = homography_result
            
            # Step 5: Scale compensation
            print("üìè Scale compensation...")
            
            # Scale matrix t·ª´ target original ‚Üí target normalized
            target_to_norm = np.array([
                [target_scale, 0, 0],
                [0, target_scale, 0],
                [0, 0, 1]
            ], dtype=np.float32)
            
            # Scale matrix t·ª´ base normalized ‚Üí base original  
            norm_to_base = np.array([
                [1/base_scale, 0, 0],
                [0, 1/base_scale, 0], 
                [0, 0, 1]
            ], dtype=np.float32)
            
            # Final transformation matrix
            final_matrix = norm_to_base @ best_matrix @ target_to_norm
            
            # Step 6: Apply transformation
            h, w = base_image_original.shape[:2]
            aligned_image = cv2.warpPerspective(target_image_original, final_matrix, (w, h))
            
            # Step 7: Quality assessment
            quality_score = self.calculate_quality_score(base_image_original, aligned_image)
            
            # Step 8: Create visualization images
            vis_image = self.create_visualization(base_norm, target_norm, kp1, kp2, good_matches, best_mask)
            comparison = self.create_comparison_image(base_image_original, target_image_original, aligned_image)
            
            return {
                "success": True,
                "aligned_image": aligned_image,
                "visualization_image": vis_image,
                "comparison_image": comparison,
                "base_image": base_image_original,
                "target_image": target_image_original,
                "original_sizes": {
                    "base": base_image_original.shape,
                    "target": target_image_original.shape
                },
                "normalized_sizes": {
                    "base": base_norm.shape,
                    "target": target_norm.shape
                },
                "features": {"base": len(kp1), "target": len(kp2)},
                "good_matches": len(good_matches),
                "inliers": best_inliers,
                "inlier_ratio": best_inliers / len(good_matches),
                "quality_score": quality_score,
                "homography_matrix": final_matrix,
                "scales": {"base_scale": base_scale, "target_scale": target_scale}
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def print_result_summary(self, result):
        """
        In t√≥m t·∫Øt k·∫øt qu·∫£ alignment
        
        Args:
            result (dict): K·∫øt qu·∫£ t·ª´ h√†m align()
        """
        if result["success"]:
            print("\n" + "="*50)
            print("‚úÖ SIZE-NORMALIZED ALIGNMENT TH√ÄNH C√îNG!")
            
            print(f"\nüìè Size Info:")
            print(f"  Original - Base: {result['original_sizes']['base']}, Target: {result['original_sizes']['target']}")
            print(f"  Normalized - Base: {result['normalized_sizes']['base']}, Target: {result['normalized_sizes']['target']}")
            print(f"  Scales - Base: {result['scales']['base_scale']:.3f}, Target: {result['scales']['target_scale']:.3f}")
            
            print(f"\nüîç Matching Info:")
            print(f"  Features: Base={result['features']['base']}, Target={result['features']['target']}")
            print(f"  Good matches: {result['good_matches']}")
            print(f"  Inliers: {result['inliers']} (ratio: {result['inlier_ratio']:.3f})")
            print(f"  Quality Score: {result['quality_score']:.3f}")
            
            if result['quality_score'] > 0.7:
                print("üéâ CH·∫§T L∆Ø·ª¢NG XU·∫§T S·∫ÆC!")
            elif result['quality_score'] > 0.5:
                print("üëç CH·∫§T L∆Ø·ª¢NG T·ªêT!")
            elif result['quality_score'] > 0.3:
                print("üëå Ch·∫•t l∆∞·ª£ng kh√°")
            else:
                print("‚ö†Ô∏è C·∫ßn ki·ªÉm tra")
                
        else:
            print(f"‚ùå L·ªói: {result['error']}")
    
    def display_results(self, result):
        """
        Hi·ªÉn th·ªã k·∫øt qu·∫£ alignment b·∫±ng matplotlib
        
        Args:
            result (dict): K·∫øt qu·∫£ t·ª´ h√†m align()
        """
        if not result["success"]:
            print(f"‚ùå L·ªói: {result['error']}")
            return
        
        # Convert BGR to RGB cho matplotlib
        aligned_rgb = cv2.cvtColor(result["aligned_image"], cv2.COLOR_BGR2RGB)
        vis_rgb = cv2.cvtColor(result["visualization_image"], cv2.COLOR_BGR2RGB)
        comp_rgb = cv2.cvtColor(result["comparison_image"], cv2.COLOR_BGR2RGB)
        
        # Display aligned image
        plt.figure(figsize=(8, 8))
        plt.title("Aligned Image")
        plt.imshow(aligned_rgb)
        plt.axis("off")
        plt.tight_layout()
        plt.show()
        
        # Display visualization
        plt.figure(figsize=(14, 7))
        plt.title("Feature Matches Visualization")
        plt.imshow(vis_rgb)
        plt.axis("off")
        plt.tight_layout()
        plt.show()
        
        # Display comparison
        plt.figure(figsize=(18, 6))
        plt.title("Comparison (Base | Target | Aligned)")
        plt.imshow(comp_rgb)
        plt.axis("off")
        plt.tight_layout()
        plt.show()


# Example usage
if __name__ == "__main__":
    # Kh·ªüi t·∫°o aligner
    aligner = ORBImageAligner(target_dimension=800, orb_features=2000)
    
    # Th·ª±c hi·ªán alignment (c√≥ th·ªÉ truy·ªÅn ƒë∆∞·ªùng d·∫´n ho·∫∑c numpy array)
    base_path = r"C:\WorkSpace\DECDM\img\base_cccd_qr.jpg"
    target_path = r"C:\Users\0100644068\Downloads\New folder (3)\z7065551240806_9bc3b22dba7439d88d3d49b02e3f85eb.jpg"
    
    print("üöÄ SIZE-NORMALIZED ORB ALIGNMENT (MEMORY ONLY)")
    print("="*50)
    
    result = aligner.align(base_path, target_path)
    
    # In t√≥m t·∫Øt
    aligner.print_result_summary(result)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    if result["success"]:
        aligner.display_results(result)
        
        # N·∫øu mu·ªën l∆∞u file (optional)
        # cv2.imwrite("aligned.jpg", result["aligned_image"])
        # cv2.imwrite("visualization.jpg", result["visualization_image"])
        # cv2.imwrite("comparison.jpg", result["comparison_image"])