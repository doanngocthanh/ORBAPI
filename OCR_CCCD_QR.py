from service.yolo.YOLODetector import YOLODetector, DetectionConfig
from config import PtConfig, ImageBaseConfig
from VietOCRApi import VietOCRProcessor
import json
import numpy as np

class OCR_CCCD_QR:
    def __init__(self,face=None):
        self.config = DetectionConfig(
            conf_threshold=0.25,
            iou_threshold=0.3,
            max_positions_per_label=2,
            target_size=640,
            enhance_image=False
        )
        self.ptconfig = PtConfig()
        self.image_base_config = ImageBaseConfig()
        self.viet_ocr_processor = VietOCRProcessor()
        self.model = YOLODetector(self.ptconfig.get_model("OCR_QR_CCCD"), self.config)
        
        if face=="cccd_qr_back":
            self.image_front = self.image_base_config.get_image("base_qr_cccd_back")
        self.image_front = self.image_base_config.get_image("base_qr_cccd")
        #self.image_back = self.image_base_config.get_image("base_qr_cccd_back")
    
    def crop_black_padding(self, aligned_image):
        """
        Ch·ªâ crop b·ªè ph·∫ßn padding ƒëen xung quanh, gi·ªØ nguy√™n n·ªôi dung v√† aspect ratio
        
        Args:
            aligned_image: ·∫¢nh ƒë√£ align (PIL Image ho·∫∑c numpy array)
            
        Returns:
            numpy.ndarray: ·∫¢nh ƒë√£ crop, gi·ªØ nguy√™n k√≠ch th∆∞·ªõc n·ªôi dung th·ª±c
        """
        import cv2
        from PIL import Image
        
        # Convert to CV2 format
        if isinstance(aligned_image, Image.Image):
            aligned_cv = cv2.cvtColor(np.array(aligned_image), cv2.COLOR_RGB2BGR)
        else:
            aligned_cv = aligned_image
        
        aligned_h, aligned_w = aligned_cv.shape[:2]
        print(f"üìê Aligned image size (before crop): {aligned_w}x{aligned_h}")
        
        # T√¨m v√πng n·ªôi dung (non-black area) trong aligned image
        gray = cv2.cvtColor(aligned_cv, cv2.COLOR_BGR2GRAY)
        # T√¨m c√°c pixel kh√¥ng ƒëen (threshold > 10 ƒë·ªÉ tr√°nh noise)
        _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)
        
        # T√¨m contours c·ªßa v√πng n·ªôi dung
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            # T√¨m bounding box c·ªßa v√πng n·ªôi dung l·ªõn nh·∫•t
            largest_contour = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest_contour)
            
            # Th√™m margin nh·ªè (2 pixels) ƒë·ªÉ tr√°nh m·∫•t vi·ªÅn
            margin = 2
            x = max(0, x - margin)
            y = max(0, y - margin)
            w = min(aligned_cv.shape[1] - x, w + 2*margin)
            h = min(aligned_cv.shape[0] - y, h + 2*margin)
            
            print(f"‚úÇÔ∏è Cropping black padding: from ({aligned_w}x{aligned_h}) to ({w}x{h})")
            print(f"   Removed: left={x}px, top={y}px, right={aligned_w-x-w}px, bottom={aligned_h-y-h}px")
            
            # Crop v√πng n·ªôi dung - GI·ªÆ NGUY√äN K√çCH TH∆Ø·ªöC N·ªòI DUNG
            cropped = aligned_cv[y:y+h, x:x+w]
            
            print(f"‚úÖ Final size after crop: {cropped.shape[1]}x{cropped.shape[0]}")
            return cropped
        else:
            print("‚ö†Ô∏è Could not find content area, returning original aligned image")
            return aligned_cv
    
    def process_image(self, image_path):
        """
        Process citizen card image and extract text data
        
        Args:
            image_path: Path to image or np.ndarray
            
        Returns:
            dict: Extracted citizen card data
        """
        result = self.model.detect(image_path)
        citizens_card_data = {}
        expected_detections = len(result) # Expected number of labels for a complete citizen card
        print(f"Detections found: {expected_detections}")
        print(f"Total class counts: {self.model.get_total_classes()}")
        
        # Bi·∫øn ƒë·ªÉ l∆∞u ·∫£nh ƒë∆∞·ª£c s·ª≠ d·ª•ng cu·ªëi c√πng (c√≥ th·ªÉ l√† ·∫£nh g·ªëc ho·∫∑c aligned)
        final_image_for_ocr = image_path
        
        missing_detections = self.model.get_total_classes() - expected_detections
        if missing_detections > 3:
            # Re-detect using ORB if missing more than 3 fields
            print(f"Missing {missing_detections} detections, re-detecting with ORB...")
            from service.orb.ORBImageAligner import ORBImageAligner
            from PIL import Image
            import cv2
            
            aligner = ORBImageAligner(target_dimension=800, orb_features=5000)
            # Determine if image_path is front or back side and set appropriate template
            template_image = self.image_front
            alignment_result = aligner.align(template_image, image_path)
            aligned_image = alignment_result.get("aligned_image")
            
            # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng alignment
            inliers = alignment_result.get("inliers", 0)
            good_matches = alignment_result.get("good_matches", 0)
            
            print(f"üìä Alignment quality check:")
            print(f"  - Good matches: {good_matches}")
            print(f"  - Inliers: {inliers}")
            
            # Ki·ªÉm tra ƒë·ªô m·ªù (blur) c·ªßa aligned image
            if aligned_image is not None:
                if isinstance(aligned_image, Image.Image):
                    aligned_cv = cv2.cvtColor(np.array(aligned_image), cv2.COLOR_RGB2BGR)
                else:
                    aligned_cv = aligned_image
                
                gray = cv2.cvtColor(aligned_cv, cv2.COLOR_BGR2GRAY)
                blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()
                print(f"  - Blur score: {blur_score:.2f} (higher is sharper)")
                
                # Ng∆∞·ª°ng ch·∫•t l∆∞·ª£ng alignment (ƒëi·ªÅu ch·ªânh d·ª±a tr√™n th·ª±c t·∫ø)
                # S·ª≠ d·ª•ng scoring system linh ho·∫°t thay v√¨ hard threshold
                
                # 1. Ki·ªÉm tra ng∆∞·ª°ng t·ªëi thi·ªÉu tuy·ªát ƒë·ªëi (MUST HAVE)
                # Gi·∫£m ng∆∞·ª°ng v√¨ algorithm m·ªõi c√≥ th·ªÉ cho inliers th·∫•p nh∆∞ng v·∫´n t·ªët
                min_absolute_inliers = 25  # T·ªëi thi·ªÉu 25 inliers
                min_absolute_matches = 50  # TƒÉng matches v√¨ c√≥ nhi·ªÅu features h∆°n
                min_blur_score = 50  # Gi·ªØ nguy√™n blur score
                
                if inliers < min_absolute_inliers or good_matches < min_absolute_matches or blur_score < min_blur_score:
                    print(f"‚ùå Alignment quality below absolute minimum thresholds")
                    print(f"   (inliers={inliers}<{min_absolute_inliers} OR matches={good_matches}<{min_absolute_matches} OR blur={blur_score:.2f}<{min_blur_score})")
                    quality_ok = False
                else:
                    # 2. ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng b·∫±ng scoring system
                    score = 0
                    
                    # ƒêi·ªÉm cho inliers (0-40 ƒëi·ªÉm) - ƒëi·ªÅu ch·ªânh tiers
                    if inliers >= 100:
                        score += 40
                    elif inliers >= 60:
                        score += 35
                    elif inliers >= 40:
                        score += 25
                    elif inliers >= 25:
                        score += 15
                    else:
                        score += 5
                    
                    # ƒêi·ªÉm cho good matches (0-30 ƒëi·ªÉm) - tƒÉng ng∆∞·ª°ng do c√≥ nhi·ªÅu features h∆°n
                    if good_matches >= 300:
                        score += 30
                    elif good_matches >= 150:
                        score += 25
                    elif good_matches >= 80:
                        score += 20
                    elif good_matches >= 50:
                        score += 12
                    else:
                        score += 5
                    
                    # ƒêi·ªÉm cho blur score (0-30 ƒëi·ªÉm)
                    if blur_score >= 300:
                        score += 30
                    elif blur_score >= 200:
                        score += 25
                    elif blur_score >= 100:
                        score += 15
                    else:
                        score += 10
                    
                    # Ng∆∞·ª°ng ch·∫•p nh·∫≠n: >= 50/100 ƒëi·ªÉm
                    min_total_score = 50
                    quality_ok = score >= min_total_score
                    
                    print(f"  - Quality score: {score}/100 (min: {min_total_score})")
                    print(f"    ‚Ä¢ Inliers: {inliers} (weight: 40%)")
                    print(f"    ‚Ä¢ Good matches: {good_matches} (weight: 30%)")
                    print(f"    ‚Ä¢ Blur score: {blur_score:.2f} (weight: 30%)")
                
                if quality_ok:
                    print("‚úÖ Aligned image quality is acceptable, processing...")
                    
                    # Ch·ªâ crop b·ªè padding ƒëen, gi·ªØ nguy√™n k√≠ch th∆∞·ªõc n·ªôi dung
                    print("\nüîß Post-processing aligned image...")
                    processed_aligned = self.crop_black_padding(aligned_image)
                    
                    # Detect tr√™n ·∫£nh ƒë√£ x·ª≠ l√Ω
                    result = self.model.detect(processed_aligned)
                
                    # So s√°nh s·ªë l∆∞·ª£ng detections
                    detections_aligned = len(result)
                    print(f"\nüìä Comparison: Original={expected_detections}, Aligned={detections_aligned}")
                    
                    # Ch·ªâ d√πng aligned image n·∫øu detect ƒë∆∞·ª£c nhi·ªÅu h∆°n
                    if detections_aligned > expected_detections:
                        print("‚úÖ Aligned image gives better results, using it")
                        final_image_for_ocr = processed_aligned  # D√πng ·∫£nh aligned ƒë√£ x·ª≠ l√Ω cho OCR
                        
                        # Optional: Show aligned image
                        # import matplotlib.pyplot as plt
                        # plt.figure(figsize=(12, 6))
                        
                        # # Show original aligned (tr∆∞·ªõc khi crop/resize)
                        # plt.subplot(1, 2, 1)
                        # if isinstance(aligned_image, Image.Image):
                        #     plt.imshow(aligned_image)
                        # else:
                        #     plt.imshow(Image.fromarray(cv2.cvtColor(aligned_cv, cv2.COLOR_BGR2RGB)))
                        # plt.title(f"Aligned (before crop/resize)")
                        # plt.axis('off')
                        
                        # # Show processed aligned (sau khi crop/resize)
                        # plt.subplot(1, 2, 2)
                        # plt.imshow(cv2.cvtColor(processed_aligned, cv2.COLOR_BGR2RGB))
                        # plt.title(f"Processed (Detections: {detections_aligned})")
                        # plt.axis('off')
                        
                        # plt.tight_layout()
                        # plt.show()
                    else:
                        print("‚ö†Ô∏è Aligned image doesn't improve detection count, using original image")
                else:
                    print(f"‚ùå Aligned image quality is insufficient")
                    print("‚ö†Ô∏è Using original image instead")
            else:
                print("‚ùå Alignment failed, using original image")
           
          
        # Extract text from detections using the final image
        for detection in result:
            print(detection)
            class_name = detection.class_name
            if class_name not in ['portrait', 'qr_code']:
                citizens_card_data[class_name] = self.viet_ocr_processor.process_bbox(
                    final_image_for_ocr, detection.bbox
                ).get("text", "")
        
        return citizens_card_data

if __name__ == "__main__":
    ocr_processor = OCR_CCCD_QR()
    image = r"C:\Users\dntdo\Downloads\6ea2c189-978c-486b-8503-85f3ef114eef.jpg"
    result = ocr_processor.process_image(image)
    print(json.dumps(result, indent=4, ensure_ascii=False))
